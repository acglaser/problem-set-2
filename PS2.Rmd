---
title: "PS2"
author: "Audrey Glaser"
date: "1/30/2020"
output: pdf_document
---

```{r setup, include=FALSE}
library(readr)
nes2008 <- read_csv("Desktop/problem-set-2-master/nes2008.csv")
library(tidyverse)
library(ISLR)
library(broom)
library(rsample)
library(rcfss)
library(yardstick)
library(broom)
library(modelr)
library(skimr)
```

## Question 1

```{r, echo = F}
#Fit a linear regression model to the entire dataset
biden_lm <- nes2008 %>% lm(data = ., biden ~ female + age + educ + dem + rep)
summary(biden_lm)

#Estimate the MSE for the model
biden_mse <- mse(biden_lm, nes2008)
biden_mse
```

Looking at the P-values of the coefficients (using a one-tailed t-test), the 'female' variable and both party variables are significant, but the age and education variables are not, using an alpha value of 0.01. The inclusion of two extraneous variables inflates the standard errors of the correctly specified variable's coefficients.

According to our model, a female is more likely to score Biden 4.1 points high on average on the feeling thermometer. A Democrat would score Biden 15.4 points higher than independent, on average. And a Republican would score Biden 15.9 points lower than an indendent, on average.

The estimated MSE for the model (or the mean of the squared distances between the observed Biden thermometer values and predicted thermometer values) is 395.27. If the model was a perfect fit to the observed data, the MSE would be zero. 

## Question 2

### Split the sample set into a training set (50%) and a holdout set (50%).

```{r}
#Set seed
set.seed(242)

#Basic 50/50 split
nes2008_split <- initial_split(data = nes2008, 
                            prop = 0.5)

#Set each half as training and test
nes2008_train <- training(nes2008_split)
nes2008_test <- testing(nes2008_split)
```

### Fit the linear regression model using only the training observations.

```{r, echo = F}
biden_training_lm <- nes2008_train %>% lm(biden ~ female + age + educ + dem + rep, data = .)
tidy(biden_training_lm)
```

### Calculate the MSE using only the test set observations.

```{r, echo = F}
biden_test_MSE <- mse(biden_training_lm, nes2008_test)
biden_test_MSE - biden_mse
```

###  How does this value compare to the training MSE from question 1? 

The MSE for model trained on just the test data is equal to 414.899, which is 19.623 units larger than the MSE of the model trained on the entire dataset. 

This difference is expected, in part because the second model is trained on a smaller quantity of data than the first model, and in part because the second model is tested on out-of-sample data (data it hasn't "seen" before, that is.) We generally expect models to be overfitted to the data they're trained on, which is why holdout methods are useful to minimizing a model's reducible error.

## Question 3

```{r, echo = F}
set.seed(5)

#Create vector for MSE sampling distribution
mse <- vector("double", 1000)

#Iterate repeated holdout
for(i in 1:1000){ 
  train = sample(1:nrow(nes2008),0.5*nrow(nes2008))
  test = setdiff(1:nrow(nes2008),train)
  mod <- lm(biden ~ female + age + educ + dem + rep, 
                           data=nes2008[train,])
  pred <- predict(mod,nes2008[test,])
  x <- nes2008$biden[test]-pred
  mse[i] <- mean(x*x)
}

mean(mse)

#Histogram plot of MSE sampling distribution
mse_histo <- as.data.frame(mse)
ggplot(mse_histo, aes(mse)) +
  geom_histogram(binwidth = 1, alpha = 0.25) +
  geom_vline(aes(xintercept = mean(mse), color = "Sampling mean"), size = .5) +
  labs(x = "MSEs",
       y = "Count")
```

The test set MSEs generated from the 1,000 models centers around 398.6048, which is close to the
original model's MSE of 395.2702. 

### Question 4

```{r, echo = F}
#Create a function that will convert bootstrap sample into dataframe and fit a model to it
lm_coefs <- function(splits, ...) {
  ## use `analysis` or `as.data.frame` to get the analysis data
  mod <- lm(..., data = analysis(splits))
  tidy(mod)
}

#Generate your bootstraps and map the "lm_coef" function to each one
nes_boot <- nes2008 %>%
  bootstraps(1000) %>%
    mutate(coef = map(splits, lm_coefs, as.formula(biden ~ female + age + educ + dem + rep)))

#Retrieve summary dataframe of mean boostrap coef estimates 
biden_boot_lm_df <- nes_boot %>%
  unnest(coef) %>%
  group_by(term) %>%
  summarize(boot.estimate = mean(estimate),
            boot.se = sd(estimate, na.rm = TRUE))

#Retrieve summary dataframe of sample coefs
biden_lm_df <- tidy(biden_lm)

#Join dataframes for side by side comparison
biden_lm_df <- biden_lm_df %>%
  left_join(biden_boot_lm_df, by = "term") %>%
  select(c("term","boot.estimate", "estimate", "boot.se", "std.error")) 

biden_lm_df

hist(nes2008$age)
```

The beta coefficients generated by the linear model and the bootstrap model are nearly identical to each
other. With regards to standard errors, the bootstrap approach generates slightly higher standard errors than the linear model, except for the intercept and the education variable. The similar estimates of both approaches makes sense because the distributional assumptions of the linear model (that the residuals are normally distributed) are valid in this case, meaning bootstrapping is unlikely to produce significantly more accurate estimates. 
